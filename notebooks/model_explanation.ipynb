{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Skin Lesion Classification Model Explanations\n",
        "\n",
        "This notebook demonstrates the implementation of explainable AI techniques (LIME and SHAP) for the skin lesion classification model. We'll train a CNN model based on the architecture described in the paper \"Skin lesion classification of dermoscopic images using machine learning and convolutional neural network\" and explain its predictions.\n"
      ],
      "metadata": {
        "id": "hbaUbGl0AUg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import random\n",
        "import seaborn as sns\n",
        "from skimage.segmentation import mark_boundaries"
      ],
      "metadata": {
        "id": "Jgkl37wvAXbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For explainability\n",
        "import lime\n",
        "from lime import lime_image\n",
        "import shap"
      ],
      "metadata": {
        "id": "DYF7TGJ1Aae1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the project root directory to the Python path\n",
        "sys.path.append('..')"
      ],
      "metadata": {
        "id": "VGsdYx7tAcYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import project modules\n",
        "from XAI.config import CLASS_NAMES, MODEL_INPUT_SIZE, RANDOM_SEED, BATCH_SIZE\n",
        "from XAI.modeling.model import SkinLesionCNN\n",
        "from XAI.dataset import prepare_data, get_transforms"
      ],
      "metadata": {
        "id": "pcid_iwCAeHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set plotting style\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ],
      "metadata": {
        "id": "rlkwcsGoAfqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=RANDOM_SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()"
      ],
      "metadata": {
        "id": "a1hcmPv9AhK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load and Prepare Data"
      ],
      "metadata": {
        "id": "_dBK0sNaAl0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "train_loader, val_loader, test_loader = prepare_data(balanced=True)"
      ],
      "metadata": {
        "id": "fgFn8tzAAjc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the number of batches in each loader\n",
        "print(f\"Number of batches in training set: {len(train_loader)}\")\n",
        "print(f\"Number of batches in validation set: {len(val_loader)}\")\n",
        "print(f\"Number of batches in test set: {len(test_loader)}\")"
      ],
      "metadata": {
        "id": "g5R7L6M1AofZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View a batch of training data\n",
        "def show_batch(loader, num_samples=9):\n",
        "    # Get a batch of data\n",
        "    images, labels = next(iter(loader))\n",
        "\n",
        "    # Convert from tensor to numpy for visualization\n",
        "    images = images[:num_samples].cpu().numpy()\n",
        "    labels = labels[:num_samples].cpu().numpy()\n",
        "\n",
        "    # Denormalize images for display\n",
        "    images = np.transpose(images, (0, 2, 3, 1))\n",
        "    images = images * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "    images = np.clip(images, 0, 1)\n",
        "\n",
        "    # Plot images\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        axes[i].imshow(images[i])\n",
        "        axes[i].set_title(f\"Class: {list(CLASS_NAMES.values())[labels[i]]}\")\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_batch(train_loader)"
      ],
      "metadata": {
        "id": "XJxOGJEfAqL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model Creation and Training"
      ],
      "metadata": {
        "id": "O4IJ-bzdAvpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "MSVGQt3lAtsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model = SkinLesionCNN().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "YVPxnCaJAzUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training hyperparameters\n",
        "num_epochs = 30  # Reduced for demonstration\n",
        "learning_rate = 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)"
      ],
      "metadata": {
        "id": "6WzXLnM4A08_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training and validation functions\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in tqdm(loader, desc=\"Training\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "OhMS2oR5A3jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(loader, desc=\"Validating\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Calculate metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "i7FUaUQXA7nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs):\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Adjust learning rate\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Save metrics\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), '../models/best_model.pth')\n",
        "            print(f\"Model saved with val_acc: {val_acc:.4f}\")\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "jR-StL_jA98-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory for model checkpoints if it doesn't exist\n",
        "os.makedirs('../models', exist_ok=True)"
      ],
      "metadata": {
        "id": "eWYrmBgQBAp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs)"
      ],
      "metadata": {
        "id": "Gutyk-EkBDgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_loss'], label='Train')\n",
        "plt.plot(history['val_loss'], label='Validation')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['train_acc'], label='Train')\n",
        "plt.plot(history['val_acc'], label='Validation')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aIH4f_hXBF1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "best_model_path = '../models/best_model.pth'\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "MA3e5ChlBJea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "test_loss, test_acc = validate(model, test_loader, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "ItmWdOa4BLIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Explanations with LIME"
      ],
      "metadata": {
        "id": "vme4Zi7PBPhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of test images\n",
        "test_images, test_labels = next(iter(test_loader))\n",
        "test_images = test_images.to(device)\n",
        "test_labels = test_labels.to(device)"
      ],
      "metadata": {
        "id": "LSiupHjHBM1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(test_images)\n",
        "    _, predicted = torch.max(outputs, 1)"
      ],
      "metadata": {
        "id": "Re_CcbLDBRns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display some test images and their predictions\n",
        "num_images_to_show = 6\n",
        "plt.figure(figsize=(15, 10))"
      ],
      "metadata": {
        "id": "7DxKk_ORBTRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_to_explain = []\n",
        "for i in range(num_images_to_show):\n",
        "    # Get image and convert for display\n",
        "    img = test_images[i].cpu().numpy().transpose(1, 2, 0)\n",
        "    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "    img = np.clip(img, 0, 1)\n",
        "    images_to_explain.append(img)\n",
        "\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"True: {list(CLASS_NAMES.values())[test_labels[i]]}, \\nPred: {list(CLASS_NAMES.values())[predicted[i]]}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TuxNQft-BUzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to make predictions for LIME\n",
        "def predict_fn(images):\n",
        "    # Convert images to PyTorch format\n",
        "    batch = torch.stack([transforms.ToTensor()(img) for img in images])\n",
        "\n",
        "    # Normalize images\n",
        "    normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    batch = torch.stack([normalize(img) for img in batch])\n",
        "\n",
        "    # Resize images to match model input size\n",
        "    resize = transforms.Resize(MODEL_INPUT_SIZE)\n",
        "    batch = torch.stack([resize(img.unsqueeze(0)).squeeze(0) for img in batch])\n",
        "\n",
        "    batch = batch.to(device)\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(batch)\n",
        "        probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs"
      ],
      "metadata": {
        "id": "v5uDYcksBWkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LIME explainer\n",
        "explainer = lime_image.LimeImageExplainer()"
      ],
      "metadata": {
        "id": "EIw9Q4SpBYnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to explain an image with LIME\n",
        "def explain_with_lime(image, explainer, predict_fn, num_samples=1000):\n",
        "    explanation = explainer.explain_instance(\n",
        "        image,\n",
        "        predict_fn,\n",
        "        top_labels=len(CLASS_NAMES),\n",
        "        hide_color=0,\n",
        "        num_samples=num_samples\n",
        "    )\n",
        "    return explanation"
      ],
      "metadata": {
        "id": "YOxzm6nTBb3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get LIME explanations for a few test images\n",
        "explanations = []\n",
        "for i, img in enumerate(images_to_explain[:3]):  # Just explain 3 images to save time\n",
        "    print(f\"Explaining image {i+1}/3...\")\n",
        "    explanation = explain_with_lime(img, explainer, predict_fn)\n",
        "    explanations.append(explanation)"
      ],
      "metadata": {
        "id": "bTbmqpA_BdkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize LIME explanations\n",
        "def show_lime_explanations(images, explanations, predictions, true_labels):\n",
        "    num_images = len(images)\n",
        "    fig, axs = plt.subplots(num_images, 3, figsize=(18, 6*num_images))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        # Original image\n",
        "        axs[i, 0].imshow(images[i])\n",
        "        axs[i, 0].set_title(f\"Original Image\\nTrue: {list(CLASS_NAMES.values())[true_labels[i]]}\\nPred: {list(CLASS_NAMES.values())[predictions[i]]}\")\n",
        "        axs[i, 0].axis('off')\n",
        "\n",
        "        # Get the prediction label\n",
        "        pred_label = predictions[i]\n",
        "\n",
        "        # Positive explanation (features supporting the prediction)\n",
        "        temp, mask = explanations[i].get_image_and_mask(\n",
        "            pred_label, positive_only=True, num_features=5, hide_rest=False\n",
        "        )\n",
        "        axs[i, 1].imshow(mark_boundaries(temp, mask))\n",
        "        axs[i, 1].set_title(f\"Positive Influence\\nHighlighting regions supporting prediction\")\n",
        "        axs[i, 1].axis('off')\n",
        "\n",
        "        # Negative explanation (features against the prediction)\n",
        "        temp, mask = explanations[i].get_image_and_mask(\n",
        "            pred_label, positive_only=False, negative_only=True, num_features=5, hide_rest=False\n",
        "        )\n",
        "        axs[i, 2].imshow(mark_boundaries(temp, mask))\n",
        "        axs[i, 2].set_title(f\"Negative Influence\\nHighlighting regions against prediction\")\n",
        "        axs[i, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "T7JnOJeOBhyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predictions and true labels\n",
        "predictions = predicted[:3].cpu().numpy()\n",
        "true_labels = test_labels[:3].cpu().numpy()"
      ],
      "metadata": {
        "id": "qSnN9WqkBjw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show explanations\n",
        "show_lime_explanations(images_to_explain[:3], explanations, predictions, true_labels)"
      ],
      "metadata": {
        "id": "OZnmzkodBkke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Explanations with SHAP"
      ],
      "metadata": {
        "id": "eWW-fzs_BoRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DeepExplainer (SHAP)\n",
        "# We need a background dataset for DeepExplainer\n",
        "background = torch.stack([test_images[i] for i in range(10)])"
      ],
      "metadata": {
        "id": "0wX9s_FGBmb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a wrapper for the model to handle SHAP inputs properly\n",
        "class ModelWrapper(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "model_wrapper = ModelWrapper(model)"
      ],
      "metadata": {
        "id": "8-7esDqcBrnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SHAP explainer\n",
        "shap_explainer = shap.DeepExplainer(model_wrapper, background)"
      ],
      "metadata": {
        "id": "F_bqfDWIBuoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a few images to explain\n",
        "images_to_explain_shap = test_images[:3]  # Just use 3 images to save time"
      ],
      "metadata": {
        "id": "js8JoTSVBwdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate SHAP values\n",
        "print(\"Generating SHAP values... (this may take a while)\")\n",
        "shap_values = shap_explainer.shap_values(images_to_explain_shap)"
      ],
      "metadata": {
        "id": "mLr5rT4-ByNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print shapes to understand the SHAP output\n",
        "print(f\"Number of classes: {len(shap_values)}\")\n",
        "print(f\"SHAP values shape for first class: {shap_values[0].shape}\")"
      ],
      "metadata": {
        "id": "GQm9qzF8B0JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert test images to numpy for visualization\n",
        "test_images_np = images_to_explain_shap.cpu().numpy()"
      ],
      "metadata": {
        "id": "YlyUoA2XB1qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize SHAP values\n",
        "def plot_shap_explanations(images, shap_values, predictions, true_labels):\n",
        "    num_images = len(images)\n",
        "    fig, axs = plt.subplots(num_images, 2, figsize=(15, 5*num_images))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        # Get the prediction for this image\n",
        "        pred_idx = predictions[i]\n",
        "        true_idx = true_labels[i]\n",
        "\n",
        "        # Original image\n",
        "        img = images[i].transpose(1, 2, 0)\n",
        "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "        img = np.clip(img, 0, 1)\n",
        "\n",
        "        axs[i, 0].imshow(img)\n",
        "        axs[i, 0].set_title(f\"Original Image\\nTrue: {list(CLASS_NAMES.values())[true_idx]}\\nPred: {list(CLASS_NAMES.values())[pred_idx]}\")\n",
        "        axs[i, 0].axis('off')\n",
        "\n",
        "        # SHAP values for the predicted class\n",
        "        shap_values_pred_class = shap_values[pred_idx][i]\n",
        "\n",
        "        # Calculate absolute SHAP values and sum across color channels\n",
        "        abs_shap = np.abs(shap_values_pred_class).sum(axis=0)\n",
        "\n",
        "        # Normalize for better visualization\n",
        "        abs_shap = abs_shap / abs_shap.max()\n",
        "\n",
        "        # Create an RGB image where intensity represents SHAP importance\n",
        "        shap_overlay = np.zeros(img.shape)\n",
        "        for c in range(3):\n",
        "            shap_overlay[:,:,c] = img[:,:,c]\n",
        "\n",
        "        # Use a colormap for better visualization\n",
        "        cmap = plt.cm.hot\n",
        "        shap_img = cmap(abs_shap)[:,:,:3]  # Drop the alpha channel\n",
        "\n",
        "        # Blend original image with SHAP visualization\n",
        "        alpha = 0.7  # transparency of the SHAP overlay\n",
        "        blended = img * (1-alpha) + shap_img * alpha\n",
        "\n",
        "        axs[i, 1].imshow(blended)\n",
        "        axs[i, 1].set_title(f\"SHAP Explanation\\nHighlighting influential regions for prediction\")\n",
        "        axs[i, 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "UddcGXHxB3Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predictions and true labels for the selected images\n",
        "with torch.no_grad():\n",
        "    outputs = model(images_to_explain_shap)\n",
        "    _, shap_predictions = torch.max(outputs, 1)"
      ],
      "metadata": {
        "id": "ouXQQhAlB7XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show SHAP explanations\n",
        "true_labels_shap = test_labels[:3].cpu().numpy()\n",
        "predictions_shap = shap_predictions.cpu().numpy()"
      ],
      "metadata": {
        "id": "tt3znd9VB9Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_shap_explanations(test_images_np, shap_values, predictions_shap, true_labels_shap)"
      ],
      "metadata": {
        "id": "IIUkP7bzB-8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a summary plot for one image\n",
        "plt.figure(figsize=(10, 8))\n",
        "shap_values_to_plot = [sv[0] for sv in shap_values]  # First image\n",
        "shap.image_plot(shap_values_to_plot, -test_images_np[0], show=False)\n",
        "plt.title(f\"SHAP Summary Plot for {list(CLASS_NAMES.values())[predictions_shap[0]]}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PL51V3WzCAyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare LIME and SHAP for the first image\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 6))"
      ],
      "metadata": {
        "id": "YemGs4P-CCKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Original image\n",
        "img0 = images_to_explain[0]\n",
        "axs[0].imshow(img0)\n",
        "axs[0].set_title(f\"Original Image\\nTrue: {list(CLASS_NAMES.values())[true_labels[0]]}\\nPred: {list(CLASS_NAMES.values())[predictions[0]]}\")\n",
        "axs[0].axis('off')"
      ],
      "metadata": {
        "id": "R0Wq6fO-CF8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LIME explanation\n",
        "pred_label = predictions[0]\n",
        "temp, mask = explanations[0].get_image_and_mask(\n",
        "    pred_label, positive_only=True, num_features=5, hide_rest=False\n",
        ")\n",
        "axs[1].imshow(mark_boundaries(temp, mask))\n",
        "axs[1].set_title(f\"LIME Explanation\\nHighlighting regions supporting prediction\")\n",
        "axs[1].axis('off')"
      ],
      "metadata": {
        "id": "DNrEKZ97CGiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SHAP explanation\n",
        "shap_values_pred_class = shap_values[pred_label][0]\n",
        "abs_shap = np.abs(shap_values_pred_class).sum(axis=0)\n",
        "abs_shap = abs_shap / abs_shap.max()\n",
        "cmap = plt.cm.hot\n",
        "shap_img = cmap(abs_shap)[:,:,:3]\n",
        "alpha = 0.7\n",
        "blended = img0 * (1-alpha) + shap_img * alpha\n",
        "axs[2].imshow(blended)\n",
        "axs[2].set_title(f\"SHAP Explanation\\nHighlighting influential regions for prediction\")\n",
        "axs[2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ms9QM_iZCIgF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}